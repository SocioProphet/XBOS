{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "import pytz\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "# import utils\n",
    "# from utils import plotly_figure\n",
    "\n",
    "import itertools\n",
    "import xbos_services_getter as xsg\n",
    "\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "import calendar\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from DataManager.DataManager import DataManager\n",
    "from Thermostat import Tstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from ray.rllib.models import FullyConnectedNetwork, Model, ModelCatalog\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL \n",
    "\n",
    "DRL is good for demand charge problem since we don't need to incorporate the demand charge cost into every reward. We will incorporate it only into the final states. \n",
    "\n",
    "State:\n",
    "- Last k indoor temperatures of all zones (For now just use current and last)\n",
    "- Last k outdoor temperatures (For now just use current)\n",
    "- Last k actions  (For now just use current)\n",
    "- Time of Month (For demand charge)\n",
    "- Max Consumption so far\n",
    "- Comfortband for t steps into the future\n",
    "- Do not exceed for t steps into the future\n",
    "- occupancy for t steps into the future\n",
    "- price t steps into future\n",
    "\n",
    "Actions: \n",
    "[0,1,2] x num_zones\n",
    "\n",
    "We limit our observation space to one month. disregarding sesonality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add random gaussian noise to all temperatures. Gaussian noise should be distributed according to our uncertainty (historic uncertainty for outdoor temperature for last years etc). \n",
    "- Comfortband/DoNotExceed should be set for one month? \n",
    "- Occupancy should have random noise added i guess. for now just assume schedule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outdoor temperature we want to find distribution:\n",
    "$$P(T_{t+1} | T_{t})$$ so that we can sample from it. \n",
    "For now we could assume:\n",
    "$$P(T_{t+1} | T_{t}) = P(\\delta t_{t+1}) $$\n",
    "which is distributed according to gaussian distribution which has the same variance as our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is this adding to MPC\n",
    "- Easier to make demand charges happen. Do not need to incorporate into objective function at every step. Will be rewarded at the end of month. \n",
    "- Will learn a much longer predictive horizon. \n",
    "- Can use more complex models for predicting indoor temperature. MPC would loose DP possibility if using mmore complex and higher order models. \n",
    "- Could learn underlying effects of occupancy/comfortband which MPC could not catch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.agents import ppo\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "\n",
    "class MyEnv(gym.Env, DataManager): \n",
    "    def __init__(self, env_config):\n",
    "        \n",
    "        super().__init__(env_config[\"building\"], env_config[\"zones\"], \n",
    "                         env_config[\"start\"], env_config[\"end\"], env_config[\"window\"])\n",
    "        self.lambda_val = env_config[\"lambda_val\"]\n",
    "\n",
    "        # assert self.zones == all zones in building. this is because of the thermal model needing other zone temperatures. \n",
    "        \n",
    "        self.curr_timestep = 0\n",
    "        \n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        self.indoor_starting_temperatures = env_config[\"indoor_starting_temperatures\"] # to get starting temperatures [last, current]\n",
    "        self.outdoor_starting_temperature = env_config[\"outdoor_starting_temperature\"]\n",
    "        \n",
    "        self.tstats = {}\n",
    "        for iter_zone in self.zones:\n",
    "            self.tstats[iter_zone] = Tstat(self.building, iter_zone, \n",
    "                                           self.indoor_starting_temperatures[iter_zone][\"current\"], \n",
    "                                          last_temperature=self.indoor_starting_temperatures[iter_zone][\"last\"])\n",
    "        \n",
    "        \n",
    "        assert 60*60 % xsg.get_window_in_sec(self.window) == 0 # window divides an hour\n",
    "        assert (self.end - self.start).total_seconds() % xsg.get_window_in_sec(self.window) == 0 # window divides the timeframe\n",
    "        \n",
    "        # the number of timesteps \n",
    "        self.num_timesteps = int((self.end - self.start).total_seconds() / xsg.get_window_in_sec(self.window))\n",
    "        \n",
    "        self.unit = env_config[\"unit\"]\n",
    "        assert self.unit == \"F\"\n",
    "                \n",
    "        # all zones current and last temperature = 2*num_zones\n",
    "        # building outside temperature -> make a class for how this behaves = 1\n",
    "        # timestep -> do one hot encoding of week, day, hour, window  \\approx 4 + 7 + 24 + 60*60 / window\n",
    "        low_bound = [32] * (2*len(self.zones) + 1) # we could use parametric temperature bounds... for now we will give negative inft reward\n",
    "        high_bound = [100] * (2*len(self.zones) + 1) # plus one for building\n",
    "        \n",
    "        low_bound += [0] * (self.num_timesteps + 1) # total timesteps plus the final timestep\n",
    "        high_bound += [1] * (self.num_timesteps + 1) # total timesteps plus the final timestep\n",
    "        \n",
    "        self.observation_space = Box(\n",
    "            low=np.array(low_bound), high=np.array(high_bound), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.curr_timestep = 0\n",
    "        \n",
    "        for iter_zone in self.zones:\n",
    "            self.tstats[iter_zone].reset(self.indoor_starting_temperatures[iter_zone][\"current\"], \n",
    "                                          last_temperature=self.indoor_starting_temperatures[iter_zone][\"last\"])            \n",
    "        self.outdoor_temperature = self.outdoor_starting_temperature\n",
    "        \n",
    "        return self.create_curr_obs()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # find what new temperature would be. use thermal model with uncertainty. use reset if exceeding \n",
    "        # do_not_exceed. can't force it to take a different action anymore. \n",
    "        \n",
    "        # update temperatures \n",
    "        for iter_zone in self.zones:\n",
    "            tstats[iter_zone].next_temperature(aciton[iter_zone])\n",
    "            self.outdoor_temperature = np.random.normal() # TODO we should make a thermostat for the outdoor temperature.\n",
    "        self.curr_timestep += 1\n",
    "        \n",
    "        # if we reach the end time. \n",
    "        if self.curr_timestep == self.num_timesteps + 1:\n",
    "            return self.create_curr_obs(), 0, True, {}\n",
    "        \n",
    "        # check that in saftey temperature band\n",
    "        for iter_zone in self.zones:\n",
    "            curr_safety = self.do_not_exceed[iter_zone].iloc[node.timestep]\n",
    "            if not (curr_safety[\"t_low\"] <= tstats[iter_zone].temperature <= curr_safety[\"t_high\"]):\n",
    "                return self.create_curr_obs(), -float('inf'), True, {} # TODO do we want to add info? \n",
    "        \n",
    "        # get reward by calling discomfort and consumption model ... \n",
    "        reward = self.get_reward(action)\n",
    "        \n",
    "        return self.create_curr_obs(), reward, False, {} # obs, reward, done, info\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        \"\"\"Get the reward for the given action with the current observation parameters.\"\"\"\n",
    "        # get discomfort across edge\n",
    "        discomfort = {}\n",
    "        for iter_zone in self.zones:\n",
    "            curr_comfortband = self.comfortband[iter_zone].iloc[self.curr_timestep]\n",
    "            curr_occupancy = self.occupancy[iter_zone].iloc[self.curr_timestep]\n",
    "            curr_tstat = self.tstats[iter_zone]\n",
    "            average_edge_temperature = (curr_tstat.temperature + curr_tstat.last_temperature)/2.\n",
    "\n",
    "            discomfort[iter_zone] = xsg.get_discomfort(\n",
    "                self.discomfort_stub, self.building, average_edge_temperature,\n",
    "                curr_comfortband[\"t_low\"], curr_comfortband[\"t_high\"], \n",
    "                curr_occupancy)\n",
    "\n",
    "        # Get consumption across edge\n",
    "        price = 1  # self.prices.iloc[root.timestep] TODO also add right unit conversion, and duration\n",
    "        consumption_cost = {self.zones[i]: price * self.hvac_consumption[self.zones[i]][action[i]] \n",
    "                           for i in range(len(self.zones))}\n",
    "        \n",
    "        cost = ((1 - self.lambda_val) * (sum(consumption_cost.values()))) + (\n",
    "                self.lambda_val * (sum(discomfort.values())))\n",
    "        return -cost\n",
    "        \n",
    "\n",
    "    def create_curr_obs(self):\n",
    "        return _create_obs(self.tstats, self.outdoor_temperature, self.curr_timestep)\n",
    "\n",
    "    def _create_obs(self, tstats, outdoor_temperature, curr_time_step):\n",
    "        obs = np.zeros(self.observation_space.low.shape)\n",
    "        idx = 0\n",
    "        for iter_zone in zones:\n",
    "            obs[idx] = tstats[iter_zone].last_temperature \n",
    "            idx += 1\n",
    "            obs[idx] = tstats[iter_zone].temperature\n",
    "            idx += 1\n",
    "        \n",
    "        obs[idx] = self.outdoor_temperature\n",
    "        idx += 1\n",
    "        obs[idx + curr_timestep] = 1\n",
    "        \n",
    "        return obs\n",
    "        \n",
    "# ray.init()\n",
    "# trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
    "#     \"env_config\": {},  # config to pass to env class\n",
    "# })\n",
    "\n",
    "# while True:\n",
    "#     print(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(year=2019, month=1, day=1).replace(tzinfo=pytz.utc)\n",
    "end = start + datetime.timedelta(days=1)\n",
    "window = \"15m\"\n",
    "building = \"avenal-animal-shelter\"\n",
    "zones = [\"hvac_zone_shelter_corridor\"]\n",
    "indoor_starting_temperatures = {iter_zone: {\"last\":70, \"current\":71} for iter_zone in zones}\n",
    "outdoor_starting_temperature = 60\n",
    "unit = \"F\"\n",
    "lambda_val = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"start\": start,\n",
    "    \"end\": end,\n",
    "    \"window\": window,\n",
    "    \"building\": building,\n",
    "    \"zones\": zones,\n",
    "    \"indoor_starting_temperatures\": indoor_starting_temperatures,\n",
    "    \"outdoor_starting_temperature\": outdoor_starting_temperature,\n",
    "    \"unit\": unit, \n",
    "    \"lambda_val\": lambda_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = MyEnv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 20:15:10,628\tWARNING worker.py:1406 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-05-07 20:15:10,630\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-07_20-15-10_18115/logs.\n",
      "2019-05-07 20:15:10,748\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:38862 to respond...\n",
      "2019-05-07 20:15:10,867\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:26349 to respond...\n",
      "2019-05-07 20:15:10,872\tINFO services.py:760 -- Starting Redis shard with 6.87 GB max memory.\n",
      "2019-05-07 20:15:10,897\tINFO services.py:1384 -- Starting the Plasma object store with 10.31 GB memory using /tmp.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': None,\n",
       " 'redis_address': '10.142.36.180:38862',\n",
       " 'object_store_address': '/tmp/ray/session_2019-05-07_20-15-10_18115/sockets/plasma_store',\n",
       " 'webui_url': None,\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-05-07_20-15-10_18115/sockets/raylet'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 20:16:49,274\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
      "2019-05-07 20:16:49,274\tINFO tune.py:211 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 15.6/34.4 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/12 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 15.6/34.4 GB\n",
      "Result logdir: /Users/daniellengyel/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})\n",
      "PENDING trials:\n",
      " - PPO_MyEnv_1_lr=0.0001:\tPENDING\n",
      " - PPO_MyEnv_2_lr=1e-06:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_MyEnv_0_lr=0.01:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 20:16:51,384\tERROR trial_runner.py:460 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 409, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 314, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/worker.py\", line 2316, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=18135, host=Daniels-MacBook-Pro-4.local)\n",
      "  File \"pyarrow/serialization.pxi\", line 448, in pyarrow.lib.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 411, in pyarrow.lib.deserialize_from\n",
      "  File \"pyarrow/serialization.pxi\", line 262, in pyarrow.lib.SerializedPyObject.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 171, in pyarrow.lib.SerializationContext._deserialize_callback\n",
      "ModuleNotFoundError: No module named 'DataManager'\n",
      "\n",
      "2019-05-07 20:16:51,397\tINFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_MyEnv_0_lr=0.01. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-05-07 20:16:51,419\tERROR trial_runner.py:460 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 409, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 314, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/worker.py\", line 2316, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=18140, host=Daniels-MacBook-Pro-4.local)\n",
      "  File \"pyarrow/serialization.pxi\", line 448, in pyarrow.lib.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 411, in pyarrow.lib.deserialize_from\n",
      "  File \"pyarrow/serialization.pxi\", line 262, in pyarrow.lib.SerializedPyObject.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 171, in pyarrow.lib.SerializationContext._deserialize_callback\n",
      "ModuleNotFoundError: No module named 'DataManager'\n",
      "\n",
      "2019-05-07 20:16:51,431\tINFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_MyEnv_1_lr=0.0001. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-05-07 20:16:51,508\tERROR trial_runner.py:460 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 409, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 314, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/worker.py\", line 2316, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=18131, host=Daniels-MacBook-Pro-4.local)\n",
      "  File \"pyarrow/serialization.pxi\", line 448, in pyarrow.lib.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 411, in pyarrow.lib.deserialize_from\n",
      "  File \"pyarrow/serialization.pxi\", line 262, in pyarrow.lib.SerializedPyObject.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 171, in pyarrow.lib.SerializationContext._deserialize_callback\n",
      "ModuleNotFoundError: No module named 'DataManager'\n",
      "\n",
      "2019-05-07 20:16:51,520\tINFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_MyEnv_2_lr=1e-06. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 15.8/34.4 GB\n",
      "Result logdir: /Users/daniellengyel/ray_results/PPO\n",
      "Number of trials: 3 ({'ERROR': 3})\n",
      "ERROR trials:\n",
      " - PPO_MyEnv_0_lr=0.01:\tERROR, 1 failures: /Users/daniellengyel/ray_results/PPO/PPO_MyEnv_0_lr=0.01_2019-05-07_20-16-4925ijjntn/error_2019-05-07_20-16-51.txt\n",
      " - PPO_MyEnv_1_lr=0.0001:\tERROR, 1 failures: /Users/daniellengyel/ray_results/PPO/PPO_MyEnv_1_lr=0.0001_2019-05-07_20-16-49y8ajrqyt/error_2019-05-07_20-16-51.txt\n",
      " - PPO_MyEnv_2_lr=1e-06:\tERROR, 1 failures: /Users/daniellengyel/ray_results/PPO/PPO_MyEnv_2_lr=1e-06_2019-05-07_20-16-49s2ahrlk_/error_2019-05-07_20-16-51.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_MyEnv_0_lr=0.01, PPO_MyEnv_1_lr=0.0001, PPO_MyEnv_2_lr=1e-06])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6c005fc209fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# try different lrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"num_workers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;34m\"env_config\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     },\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_function, checkpoint_freq, checkpoint_at_end, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_MyEnv_0_lr=0.01, PPO_MyEnv_1_lr=0.0001, PPO_MyEnv_2_lr=1e-06])"
     ]
    }
   ],
   "source": [
    "# Can also register the env creator function explicitly with:\n",
    "# register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "# ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"timesteps_total\": 10000,\n",
    "    },\n",
    "    config={\n",
    "        \"env\": MyEnv,  # or \"corridor\" if registered above\n",
    "        \"lr\": grid_search([1e-2, 1e-4, 1e-6]),  # try different lrs\n",
    "        \"num_workers\": 1,  # parallelism\n",
    "        \"env_config\": config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from ray.rllib.models import FullyConnectedNetwork, Model, ModelCatalog\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 20:17:00,723\tWARNING worker.py:1406 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Perhaps you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3f68a533b944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(redis_address, num_cpus, num_gpus, resources, object_store_memory, redis_max_memory, log_to_driver, node_ip_address, object_id_seed, local_mode, redirect_worker_output, redirect_output, ignore_reinit_error, num_redis_shards, redis_max_clients, redis_password, plasma_directory, huge_pages, include_webui, driver_id, configure_logging, logging_level, logging_format, plasma_store_socket_name, raylet_socket_name, temp_dir, load_code_from_local, _internal_config)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             raise Exception(\"Perhaps you called ray.init twice by accident? \"\n\u001b[0m\u001b[1;32m   1417\u001b[0m                             \u001b[0;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m                             \u001b[0;34m\"'ignore_reinit_error=True' or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Perhaps you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCorridor(gym.Env):\n",
    "\n",
    "    def __init__(self, start, end, window, config):\n",
    "        \n",
    "        # config has [\"t_in_past_steps, t_out_past_steps, action_past_steps\"]\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(\n",
    "            0.0, self.end_pos, shape=(1, ), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        return [self.cur_pos], 1 if done else 0, done, {}\n",
    "    \n",
    "    \n",
    "class CustomModel(Model):\n",
    "    \"\"\"Example of a custom model.\n",
    "    This model just delegates to the built-in fcnet.\n",
    "    \"\"\"\n",
    "\n",
    "    def _build_layers_v2(self, input_dict, num_outputs, options):\n",
    "        self.obs_in = input_dict[\"obs\"]\n",
    "        self.fcnet = FullyConnectedNetwork(input_dict, self.obs_space,\n",
    "                                           self.action_space, num_outputs,\n",
    "                                           options)\n",
    "        return self.fcnet.outputs, self.fcnet.last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 20:28:07,884\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
      "2019-05-07 20:28:07,885\tINFO tune.py:211 -- Starting a new experiment.\n",
      "2019-05-07 20:28:08,009\tWARNING util.py:62 -- The `start_trial` operation took 0.10692095756530762 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 15.8/34.4 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/12 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 15.8/34.4 GB\n",
      "Result logdir: /Users/daniellengyel/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})\n",
      "PENDING trials:\n",
      " - PPO_SimpleCorridor_1_lr=0.0001:\tPENDING\n",
      " - PPO_SimpleCorridor_2_lr=1e-06:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_SimpleCorridor_0_lr=0.01:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 20:28:10,046\tERROR trial_runner.py:460 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 409, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 314, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/worker.py\", line 2316, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=18138, host=Daniels-MacBook-Pro-4.local)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 276, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trainable.py\", line 88, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 373, in _setup\n",
      "    self._init()\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo.py\", line 77, in _init\n",
      "    self.env_creator, self._policy_graph)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 506, in make_local_evaluator\n",
      "    extra_config or {}))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 714, in _make_evaluator\n",
      "    async_remote_worker_envs=config[\"async_remote_worker_envs\"])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 230, in __init__\n",
      "    self.env = _validate_env(env_creator(env_context))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 752, in <lambda>\n",
      "    register_env(name, lambda config: env_object(config))\n",
      "TypeError: __init__() missing 3 required positional arguments: 'end', 'window', and 'config'\n",
      "\n",
      "2019-05-07 20:28:10,050\tINFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_SimpleCorridor_0_lr=0.01. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-05-07 20:28:10,056\tERROR trial_runner.py:460 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 409, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 314, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/worker.py\", line 2316, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=18132, host=Daniels-MacBook-Pro-4.local)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 276, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trainable.py\", line 88, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 373, in _setup\n",
      "    self._init()\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo.py\", line 77, in _init\n",
      "    self.env_creator, self._policy_graph)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 506, in make_local_evaluator\n",
      "    extra_config or {}))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 714, in _make_evaluator\n",
      "    async_remote_worker_envs=config[\"async_remote_worker_envs\"])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 230, in __init__\n",
      "    self.env = _validate_env(env_creator(env_context))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 752, in <lambda>\n",
      "    register_env(name, lambda config: env_object(config))\n",
      "TypeError: __init__() missing 3 required positional arguments: 'end', 'window', and 'config'\n",
      "\n",
      "2019-05-07 20:28:10,060\tINFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_SimpleCorridor_1_lr=0.0001. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-05-07 20:28:10,068\tERROR trial_runner.py:460 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 409, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 314, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/worker.py\", line 2316, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=18139, host=Daniels-MacBook-Pro-4.local)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 276, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/trainable.py\", line 88, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 373, in _setup\n",
      "    self._init()\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo.py\", line 77, in _init\n",
      "    self.env_creator, self._policy_graph)\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 506, in make_local_evaluator\n",
      "    extra_config or {}))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 714, in _make_evaluator\n",
      "    async_remote_worker_envs=config[\"async_remote_worker_envs\"])\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 230, in __init__\n",
      "    self.env = _validate_env(env_creator(env_context))\n",
      "  File \"/Users/daniellengyel/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/rllib/agents/agent.py\", line 752, in <lambda>\n",
      "    register_env(name, lambda config: env_object(config))\n",
      "TypeError: __init__() missing 3 required positional arguments: 'end', 'window', and 'config'\n",
      "\n",
      "2019-05-07 20:28:10,073\tINFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_SimpleCorridor_2_lr=1e-06. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=18138)\u001b[0m 2019-05-07 20:28:10,042\tWARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 16.0/34.4 GB\n",
      "Result logdir: /Users/daniellengyel/ray_results/PPO\n",
      "Number of trials: 3 ({'ERROR': 3})\n",
      "ERROR trials:\n",
      " - PPO_SimpleCorridor_0_lr=0.01:\tERROR, 1 failures: /Users/daniellengyel/ray_results/PPO/PPO_SimpleCorridor_0_lr=0.01_2019-05-07_20-28-07juldidjd/error_2019-05-07_20-28-10.txt\n",
      " - PPO_SimpleCorridor_1_lr=0.0001:\tERROR, 1 failures: /Users/daniellengyel/ray_results/PPO/PPO_SimpleCorridor_1_lr=0.0001_2019-05-07_20-28-08xpf725ru/error_2019-05-07_20-28-10.txt\n",
      " - PPO_SimpleCorridor_2_lr=1e-06:\tERROR, 1 failures: /Users/daniellengyel/ray_results/PPO/PPO_SimpleCorridor_2_lr=1e-06_2019-05-07_20-28-08jlehu0ik/error_2019-05-07_20-28-10.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_SimpleCorridor_0_lr=0.01, PPO_SimpleCorridor_1_lr=0.0001, PPO_SimpleCorridor_2_lr=1e-06])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a38f71cc55e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         },\n\u001b[1;32m     19\u001b[0m     },\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mreuse_actors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_function, checkpoint_freq, checkpoint_at_end, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_SimpleCorridor_0_lr=0.01, PPO_SimpleCorridor_1_lr=0.0001, PPO_SimpleCorridor_2_lr=1e-06])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=18139)\u001b[0m 2019-05-07 20:28:10,062\tWARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=18132)\u001b[0m 2019-05-07 20:28:10,050\tWARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n"
     ]
    }
   ],
   "source": [
    "# Can also register the env creator function explicitly with:\n",
    "# register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"timesteps_total\": 10000,\n",
    "    },\n",
    "    config={\n",
    "        \"env\": SimpleCorridor,  # or \"corridor\" if registered above\n",
    "        \"model\": {\n",
    "            \"custom_model\": \"my_model\",\n",
    "        },\n",
    "        \"lr\": grid_search([1e-2, 1e-4, 1e-6]),  # try different lrs\n",
    "        \"num_workers\": 1,  # parallelism\n",
    "        \"env_config\": {\n",
    "            \"corridor_length\": 5,\n",
    "        },\n",
    "    },\n",
    "    reuse_actors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3982164924625597"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 3.584411859512329\n"
     ]
    }
   ],
   "source": [
    "import xbos_services_getter\n",
    "XBOS_MICROSERVICES_HOST_ADDRESS=\"ms.xbos.io:9001\"\n",
    "import time\n",
    "discomfort_stub = xbos_services_getter.get_discomfort_stub()\n",
    "a = time.time()\n",
    "for i in range(0,1000):\n",
    "    discomfort = xbos_services_getter.get_discomfort(discomfort_stub,'bldg',95,90,94,1.0)\n",
    "print(\"time\", time.time() - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ms.xbos.io:9001'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"DISCOMFORT_HOST_ADDRESS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /Users/daniellengyel/.bash_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(np.int64, int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-dr3)\n",
   "language": "python",
   "name": "venv-dr3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
