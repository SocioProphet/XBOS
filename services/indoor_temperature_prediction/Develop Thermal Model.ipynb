{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the utils file here\n",
    "import os, sys\n",
    "import xbos_services_getter as xsg\n",
    "import datetime\n",
    "import calendar\n",
    "import pytz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_indoor_data as pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_zone_names_stub = xsg.get_building_zone_names_stub()\n",
    "all_buildings_zones = xsg.get_all_buildings_zones(building_zone_names_stub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Getters\n",
    "Ground truth for how to store and retrieve data. The file naming is building_zone. No differentiation between training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(data, building, zone):\n",
    "    data_dir = Path.cwd() / \"services_data\"\n",
    "    if not os.path.isdir(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    file_path = data_dir / (building + \"_\" + zone + \".pkl\")\n",
    "    if not os.path.isfile(file_path):\n",
    "        return None\n",
    "                                \n",
    "    with open(str(file_path), \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_data(building, zone):\n",
    "    data_dir = Path.cwd() / \"services_data\"\n",
    "    if not os.path.isdir(data_dir):\n",
    "        return None\n",
    "\n",
    "    file_path = data_dir / (building + \"_\" + zone + \".pkl\")\n",
    "    if not os.path.isfile(file_path):\n",
    "        return None\n",
    "                            \n",
    "    with open(str(file_path), \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "num_start and num_end are hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building = \"avenal-animal-shelter\"\n",
    "zone = all_buildings_zones[building][0]\n",
    "prediction_window = \"5m\"\n",
    "seconds_prediction_window = xsg.get_window_in_sec(prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daterange\n",
    "start = datetime.datetime(year=2018, month=7, day=1).replace(tzinfo=pytz.utc)\n",
    "end = start + datetime.timedelta(days=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add check that the data we have stored is at least as long and has right prediction_window\n",
    "# TODO Fix how we deal with nan's. some zone temperatures might get set to -1.\n",
    "loaded_data = load_data(building, zone)\n",
    "if loaded_data is None:\n",
    "    processed_data = pid.get_preprocessed_data(building, zone, start, end, prediction_window)\n",
    "    store_data(processed_data, building, zone)\n",
    "else:\n",
    "    processed_data = loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features to prepocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pid.indoor_data_cleaning(processed_data)\n",
    "processed_data = pid.add_feature_last_temperature(processed_data)\n",
    "processed_data = pid.convert_categorical_action(processed_data, num_start=4, num_end=4, interval_thermal=seconds_prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regressor with all features\n",
    "This will use all available features and make it into a linear regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7 # how much training data to take from given data\n",
    "N = processed_data.shape[0] # number of datapoints\n",
    "train_data = processed_data.iloc[:int(N*train_ratio)]\n",
    "test_data = processed_data.iloc[int(N*train_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"action\", \"action_prev\", \"dt\", \"action_duration\"]\n",
    "\n",
    "# Training data\n",
    "train_data = train_data[train_data[\"dt\"] == seconds_prediction_window]\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "train_y = train_data[\"t_next\"].interpolate(method=\"time\")\n",
    "train_X = train_data.drop([\"t_next\"], axis=1).interpolate(method=\"time\")\n",
    "\n",
    "# Test data\n",
    "test_data = test_data[test_data[\"dt\"] == seconds_prediction_window]\n",
    "test_data = test_data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "test_y = test_data[\"t_next\"].interpolate(method=\"time\")\n",
    "test_X = test_data.drop([\"t_next\"], axis=1).interpolate(method=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "reg.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7 # how much training data to take from given data\n",
    "N = processed_data.shape[0] # number of datapoints\n",
    "train_data = processed_data.iloc[:int(N*train_ratio)]\n",
    "test_data = processed_data.iloc[int(N*train_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_drop = []\n",
    "for c in train_data.columns:\n",
    "    if \"action\" in c and len(\"action\") != c:\n",
    "        action_to_drop.append(c)\n",
    "        \n",
    "columns_to_drop = action_to_drop + [\"t_prev\", \"action_prev\", \"dt\", \"action_duration\"]\n",
    "  \n",
    "# train data\n",
    "train_data = train_data[train_data[\"dt\"] == seconds_prediction_window]\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "train_y = train_data[\"t_next\"].interpolate(method=\"time\")\n",
    "train_X = train_data.drop([\"t_next\"], axis=1).interpolate(method=\"time\")\n",
    "\n",
    "# test data\n",
    "test_data = test_data[test_data[\"dt\"] == seconds_prediction_window]\n",
    "test_data = test_data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "test_y = test_data[\"t_next\"].interpolate(method=\"time\")\n",
    "test_X = test_data.drop([\"t_next\"], axis=1).interpolate(method=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression().fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tests for Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will try to do forecasting. If it fails, won't return anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting(thermal_model, data, start, duration, seconds_prediction_window, dt, is_second_order=False):\n",
    "\n",
    "    true_data = data.loc[start:]\n",
    "    \n",
    "    if true_data.index[-1] < start + datetime.timedelta(seconds=duration) or true_data.index[0] != start:\n",
    "        return None\n",
    "    \n",
    "    forecast = []\n",
    "\n",
    "    curr_time = true_data.index[0]\n",
    "    \n",
    "    try:\n",
    "        while curr_time <= start + datetime.timedelta(seconds=duration):\n",
    "\n",
    "            curr_row = true_data.loc[curr_time].to_frame().T\n",
    "\n",
    "            if (len(forecast) < 2 and is_second_order) or (len(forecast) < 1):\n",
    "                forecast.append(float(curr_row[\"t_in\"].values))\n",
    "            else:\n",
    "                curr_row[\"t_in\"] = forecast[-1]\n",
    "                if is_second_order:\n",
    "                    curr_row[\"t_prev\"] = forecast[-2]\n",
    "                forecast.append(thermal_model.predict(curr_row)[0])\n",
    "\n",
    "            curr_time += datetime.timedelta(seconds=float(dt.loc[curr_time]))\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    forecast = forecast[:-1] # otherwise might predict beyond the set end\n",
    "    return pd.Series(index=true_data.index[:len(forecast)], data=forecast)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = processed_data.loc[test_X.index[0]:test_X.index[-1]][\"dt\"]\n",
    "\n",
    "N = test_X.shape[0]\n",
    "\n",
    "forecasts = []\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    start_time = test_X.index[i]\n",
    "    forecast = forecasting(reg, test_X, start_time, 6*60*60, 5*60, dt, is_second_order=True)\n",
    "    \n",
    "    if forecast is not None:\n",
    "        forecasts.append(forecast)\n",
    "        \n",
    "    if i % 500 == 0:\n",
    "        print(\"Iteration:\", i)\n",
    "        print(\"Successful Forecasts:\", len(forecasts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get RMSE plot\n",
    "Will do shady stuff. At least we can have duration / interval number of predictions. so we will only use that many. Otherwise dimensions don't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = []\n",
    "least_points = int(6*60*60 / (5*60))\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    forecast = forecasts[i]\n",
    "    real_data = test_X.loc[forecast.index][\"t_in\"]\n",
    "    if real_data.shape[0] >= least_points:\n",
    "        errs.append((forecast - real_data).values[:least_points])\n",
    "        \n",
    "print(\"Num before\", len(forecasts))\n",
    "print(\"Num remaining\", len(errs))\n",
    "errs = np.vstack(errs)\n",
    "errs = np.square(errs)\n",
    "errs = np.mean(errs, axis=0)\n",
    "errs = np.sqrt(errs)\n",
    "errs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occ_plot = pd.Series(index=date_range, data=test_data[\"occ\"][date_range])\n",
    "\n",
    "real_plot = test_X.loc[forecast.index][\"t_in\"]\n",
    "\n",
    "# real_outside_plot = new_pred_horizon.loc[date_range][\"t_out\"]\n",
    "\n",
    "# real_action_plt = new_pred_horizon.loc[date_range][\"action\"]\n",
    "\n",
    "# real_action_plt *= 5\n",
    "# real_action_plt += 65\n",
    "# real_action_plt.plot(label=\"action\", color=\"darkblue\")\n",
    "\n",
    "# real_outside_plot.plot(label=\"t_out\")\n",
    "\n",
    "real_plot.plot(label=\"real\", color=\"goldenrod\")\n",
    "forecast.plot(label=\"forecast\", color=\"firebrick\")\n",
    "\n",
    "\n",
    "\n",
    "# first_tm_plot.plot(label=\"First Order TM\", color=\"firebrick\")\n",
    "# plt.show()\n",
    "\n",
    "# # second_tm_plot.plot(label=\"Second order TM\", color=\"steelblue\")\n",
    "\n",
    "# lti_tm_plot.plot(label=\"LTI TM\", color=\"mediumpurple\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# random_forest_plot.plot(label=\"Random Forest\",  color=\"black\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# print(\"plot\")\n",
    "# print(occ_plot)\n",
    "\n",
    "# occ_plot.plot()\n",
    "# plt.show()\n",
    "\n",
    "def get_rmse_plot(pred, real):\n",
    "    pred = pred[:len(real)]\n",
    "    real = real[:len(pred)]\n",
    "    diff = pred - real\n",
    "    diff = np.square(diff)\n",
    "    return np.sqrt(diff.mean())\n",
    "\n",
    "# print(\"lag\", get_rmse_plot(tm_pred_plot, real_plot))\n",
    "# print(\"old\", get_rmse_plot(old_pred_plot, real_plot))\n",
    "# print(\"occ lag\", get_rmse_plot(lag_tm_plot, real_plot))\n",
    "# print(\"sec order\", get_rmse_plot(second_tm_pred, real_plot))\n",
    "# print(\"sec order occ\", get_rmse_plot(second_occ_tm_pred, real_plot))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(building, zone, start, end, prediction_window, raw_data_granularity, train_ratio, is_second_order, \n",
    "                 curr_action_timesteps, prev_action_timesteps, method, rmse_series, num_forecasts, forecasting_horizon):\n",
    "    \"\"\"Stores the results and the methods as a yamls file. The files is stores in a way that it can be used\n",
    "        to configure the exact same model and get the results. \n",
    "    \n",
    "    :param building: (string) building name\n",
    "    :param zone: (string) zone name\n",
    "    :param start: (datetime timezone aware) start of the dataset used\n",
    "    :param end: (datetime timezone aware) start of the dataset used\n",
    "    :param prediction_window: (int seconds) number of seconds between predictions\n",
    "    :param raw_data_granularity: (int seconds) the window size of the raw data. needs to be less than prediction_window.\n",
    "    :param train_ratio: (float) in (0, 1). the ratio in which to split train and test set from the given dataset. The train set comes before test set in time. \n",
    "    :param is_second_order: (bool) Whether we are using second order in temperature.\n",
    "    :param curr_action_timesteps: (int) The order of the current action. Set 0 if there should only be one action.\n",
    "    :param prev_action_timesteps: (int) The order of the previous action. Set 0 if it should not be used at all.\n",
    "    :param method: (str) [\"OLS\", \"random_forest\", \"LSTM\"] are the available methods so far\n",
    "    :param rmse_series: np.array the rmse of the forecasting procedure. \n",
    "    :param num_forecasts: (int) The number of forecasts which contributed to the RMSE. \n",
    "    :param forecasting_horizon: (int seconds) The horizon used when forecasting.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    to_store = {\"building\": building,\n",
    "               \"zone\": zone, \n",
    "               \"start\": start, \n",
    "                \"end\": end,\n",
    "               \"raw_data_granularity\": raw_data_granularity,\n",
    "                \"prediction_window\": prediction_window,\n",
    "                \"train_ratio\": train_ratio,\n",
    "                \"is_second_order\": is_second_order,\n",
    "                \"curr_action_timesteps\": curr_action_timesteps,\n",
    "                \"prev_action_timesteps\": prev_action_timesteps,\n",
    "                \"method\": method,\n",
    "                \"rmse_series\": rmse_series,\n",
    "                \"num_forecasts\": num_forecasts,\n",
    "                \"forecasting_horizon\": forecasting_horizon\n",
    "               }\n",
    "    \n",
    "    data_dir = Path.cwd() / \"model_results\"\n",
    "    if not os.path.isdir(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    file_path = data_dir / (building + \"_\" + zone + \".pkl\")\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            with open(str(file_path), \"rb\") as f:\n",
    "                loaded_results = pickle.load(f)\n",
    "            loaded_results.append(to_store)\n",
    "            to_store = loaded_results\n",
    "        except:\n",
    "            to_store = [to_store]\n",
    "    else:\n",
    "        to_store = [to_store]\n",
    "        \n",
    "    with open(str(file_path), \"wb\") as f:\n",
    "        pickle.dump(to_store, f)\n",
    "        \n",
    "def load_results(building, zone):\n",
    "    data_dir = Path.cwd() / \"model_results\"\n",
    "    if not os.path.isdir(data_dir):\n",
    "        return None\n",
    "    \n",
    "    file_path = data_dir / (building + \"_\" + zone + \".pkl\")\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        return None\n",
    "    \n",
    "    with open(str(file_path), \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(building, zone, start, end, prediction_window, 60, train_ratio, True, \n",
    "                 4, 4, \"OLS\", errs, 6167, 6*60*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_results(building, zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_automization_baby(building, zone, start, end, prediction_window, raw_data_granularity, train_ratio, is_second_order, \n",
    "                 curr_action_timesteps, prev_action_timesteps, method, rmse_series, num_forecasts, forecasting_horizon):\n",
    "    pass\n",
    "\n",
    "def create_model(building, zone, start, end, prediction_window, raw_data_granularity, train_ratio, is_second_order, \n",
    "                 curr_action_timesteps, prev_action_timesteps, method):\n",
    "    \n",
    "    if method != \"OLS\":\n",
    "        raise NotImplementedError(\"%s is not supported. Use OLS instead.\" % method)\n",
    "    \n",
    "    # Get data\n",
    "    # TODO add check that the data we have stored is at least as long and has right prediction_window\n",
    "    # TODO Fix how we deal with nan's. some zone temperatures might get set to -1.\n",
    "    loaded_data = load_data(building, zone)\n",
    "    if loaded_data is None:\n",
    "        processed_data = pid.get_preprocessed_data(building, zone, start, end, prediction_window, raw_data_granularity)\n",
    "        store_data(processed_data, building, zone)\n",
    "    else:\n",
    "        processed_data = loaded_data\n",
    "        \n",
    "    # add features\n",
    "    processed_data = pid.indoor_data_cleaning(processed_data)\n",
    "    if is_second_order:\n",
    "        processed_data = pid.add_feature_last_temperature(processed_data)\n",
    "    if curr_action_timesteps > 0 or prev_action_timesteps > 0:\n",
    "        processed_data = pid.convert_categorical_action(processed_data, num_start=curr_action_timesteps, num_end=prev_action_timesteps, interval_thermal=seconds_prediction_window)\n",
    "    \n",
    "    # split data into training and test sets\n",
    "    N = processed_data.shape[0] # number of datapoints\n",
    "    train_data = processed_data.iloc[:int(N*train_ratio)]\n",
    "    test_data = processed_data.iloc[int(N*train_ratio):]\n",
    "    \n",
    "    # which columns to drop for training and testing\n",
    "    columns_to_drop = [\"dt\", \"action_duration\"]\n",
    "    if curr_action_timesteps != 0:\n",
    "        columns_to_drop.append(\"action\")\n",
    "    if prev_action_timesteps != 0:\n",
    "        columns_to_drop.append(\"action_prev\")\n",
    "\n",
    "    # train data\n",
    "    train_data = train_data[train_data[\"dt\"] == seconds_prediction_window]\n",
    "    train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "    train_y = train_data[\"t_next\"].interpolate(method=\"time\")\n",
    "    train_X = train_data.drop([\"t_next\"], axis=1).interpolate(method=\"time\")\n",
    "\n",
    "    # test data\n",
    "    test_data = test_data[test_data[\"dt\"] == seconds_prediction_window]\n",
    "    test_data = test_data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "    test_y = test_data[\"t_next\"].interpolate(method=\"time\")\n",
    "    test_X = test_data.drop([\"t_next\"], axis=1).interpolate(method=\"time\")\n",
    "    \n",
    "    # Make OLS model\n",
    "    reg = LinearRegression().fit(train_X, train_y)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and run on all Buildings/Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building hayward-station-8\n",
      "Zone HVAC_Zone_F-2\n"
     ]
    },
    {
     "ename": "_Rendezvous",
     "evalue": "<_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"No data received from database.\"\n\tdebug_error_string = \"{\"created\":\"@1553988333.716740000\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1017,\"grpc_message\":\"No data received from database.\",\"grpc_status\":3}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_Rendezvous\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a83928f35461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         reg, p_data, test_X, test_y = ctm.create_model(bldg, zone, \n\u001b[1;32m     18\u001b[0m                              \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"5m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                              1, 1, \"OLS\")\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#             reg, p_data, test_X, test_y = ctm.create_model(bldg, zone,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BETS/XBOS/services/indoor_temperature_prediction/create_test_models.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(building, zone, start, end, prediction_window, raw_data_granularity, train_ratio, is_second_order, curr_action_timesteps, prev_action_timesteps, method)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mloaded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloaded_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preprocessed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data_granularity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mstore_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BETS/XBOS/services/indoor_temperature_prediction/process_indoor_data.py\u001b[0m in \u001b[0;36mget_preprocessed_data\u001b[0;34m(building, zone, start, end, window, raw_data_granularity)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mindoor_historic_stub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indoor_historic_stub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     indoor_temperatures = xsg.get_indoor_temperature_historic(indoor_historic_stub, building, zone, start, end,\n\u001b[0;32m---> 36\u001b[0;31m                                                              raw_data_granularity)\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mindoor_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions_historic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindoor_historic_stub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data_granularity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BETS/xbos_services_getter/xbos_services_getter/xbos_services_getter.py\u001b[0m in \u001b[0;36mget_indoor_temperature_historic\u001b[0;34m(indoor_historic_stub, building, zone, start, end, window)\u001b[0m\n\u001b[1;32m    299\u001b[0m     historic_indoor_response = indoor_historic_stub.GetRawTemperatures(\n\u001b[1;32m    300\u001b[0m         indoor_temperature_action_pb2.Request(building=building, zone=zone, start=start_unix, end=end_unix,\n\u001b[0;32m--> 301\u001b[0;31m                                               window=window))\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# process data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwith_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv-dr3/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_Rendezvous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_Rendezvous\u001b[0m: <_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"No data received from database.\"\n\tdebug_error_string = \"{\"created\":\"@1553988333.716740000\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1017,\"grpc_message\":\"No data received from database.\",\"grpc_status\":3}\"\n>"
     ]
    }
   ],
   "source": [
    "import create_test_models as ctm\n",
    "import datetime\n",
    "import pytz\n",
    "import xbos_services_getter as xsg\n",
    "\n",
    "building_zone_names_stub = xsg.get_building_zone_names_stub()\n",
    "all_building_zones = xsg.get_all_buildings_zones(building_zone_names_stub)\n",
    "\n",
    "start = datetime.datetime(year=2018, month=7, day=1).replace(tzinfo=pytz.utc)\n",
    "end = start + datetime.timedelta(days=10)\n",
    "\n",
    "\n",
    "for bldg in list(not_working.keys())[1:]:\n",
    "    print(\"Building\", bldg)\n",
    "    for zone in all_building_zones[bldg]:\n",
    "        print(\"Zone\", zone)\n",
    "        reg, p_data, test_X, test_y = ctm.create_model(bldg, zone, \n",
    "                             start, end, \"5m\", \"1m\", 0.7, True, \n",
    "                             1, 1, \"OLS\")\n",
    "#         try:\n",
    "#             reg, p_data, test_X, test_y = ctm.create_model(bldg, zone, \n",
    "#                              start, end, \"5m\", \"1m\", 0.7, True, \n",
    "#                              1, 1, \"OLS\")\n",
    "#             print(\"Score\", reg.score(test_X, test_y))\n",
    "\n",
    "#         except:\n",
    "#             if bldg not in not_working:\n",
    "#                 not_working[bldg] = []\n",
    "#             not_working[bldg].append(zone)\n",
    "    \n",
    "    print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-66361735d7b1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-66361735d7b1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    at first did 10 days\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "at first did 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_working = {'hayward-station-1': ['HVAC_Zone_AC-7',\n",
    "  'HVAC_Zone_AC-6',\n",
    "  'HVAC_Zone_AC-5',\n",
    "  'HVAC_Zone_AC-4',\n",
    "  'HVAC_Zone_AC-3',\n",
    "  'HVAC_Zone_AC-2',\n",
    "  'HVAC_Zone_AC-1'],\n",
    " 'hayward-station-8': ['HVAC_Zone_F-2', 'HVAC_Zone_F-3', 'HVAC_Zone_F-1'],\n",
    " 'north-berkeley-senior-center': ['HVAC_Zone_AC-5',\n",
    "  'HVAC_Zone_AC-3',\n",
    "  'HVAC_Zone_AC-1'],\n",
    " 'csu-dominguez-hills': ['HVAC_Zone_SAC_2134',\n",
    "  'HVAC_Zone_SAC_2113A',\n",
    "  'HVAC_Zone_SAC_2149',\n",
    "  'HVAC_Zone_SAC_2103',\n",
    "  'HVAC_Zone_SAC_2107',\n",
    "  'HVAC_Zone_SAC_2144',\n",
    "  'HVAC_Zone_Sac_2_Corridor',\n",
    "  'HVAC_Zone_SAC_2114',\n",
    "  'HVAC_Zone_SAC_2113',\n",
    "  'HVAC_Zone_SAC-2106',\n",
    "  'HVAC_Zone_SAC-2104',\n",
    "  'HVAC_Zone_SAC-2102',\n",
    "  'HVAC_Zone_SAC_2150',\n",
    "  'HVAC_Zone_SAC_2105',\n",
    "  'HVAC_Zone_SAC_2101',\n",
    "  'HVAC_Zone_SAC_2129',\n",
    "  'HVAC_Zone_SAC_2126'],\n",
    " 'orinda-community-center': ['HVAC_Zone_RM2',\n",
    "  'HVAC_Zone_RM1',\n",
    "  'HVAC_Zone_RM6',\n",
    "  'HVAC_Zone_RM7',\n",
    "  'HVAC_Zone_AC-8',\n",
    "  'HVAC_Zone_AC-7',\n",
    "  'HVAC_Zone_AC-6',\n",
    "  'HVAC_Zone_AC-5',\n",
    "  'HVAC_Zone_AC-4',\n",
    "  'HVAC_Zone_AC-3',\n",
    "  'HVAC_Zone_AC-2',\n",
    "  'HVAC_Zone_AC-1',\n",
    "  'HVAC_Zone_Kinder_GYM',\n",
    "  'HVAC_Zone_FRONT_OFFICE'],\n",
    " 'avenal-veterans-hall': ['HVAC_Zone_AC-6',\n",
    "  'HVAC_Zone_AC-5',\n",
    "  'HVAC_Zone_AC-4',\n",
    "  'HVAC_Zone_AC-3',\n",
    "  'HVAC_Zone_AC-2',\n",
    "  'HVAC_Zone_AC-1'],\n",
    " 'south-berkeley-senior-center': ['HVAC_Zone_AC-3',\n",
    "  'HVAC_Zone_Front_Office',\n",
    "  'HVAC_Zone_AC-2']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-dr3)\n",
   "language": "python",
   "name": "venv-dr3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
